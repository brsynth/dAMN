{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f6ebc3-26f0-4700-8a61-4ed078ce560f",
   "metadata": {},
   "source": [
    "# dAMN with M28 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b79d9c-a364-4175-910b-0d407998cc80",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880aae3-66b4-4e2d-959c-fbd8edceb9cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print('Physical GPUs:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "train_test_split = 'forecast' # 'forecast' or 'medium'\n",
    "folder = './'\n",
    "file_name = 'M28_OD_20'\n",
    "media_file = folder+'data/'+'M28_media.csv'                 \n",
    "od_file = folder+'data/'+file_name+'.csv'               \n",
    "cobra_model_file = folder+'data/'+'iML1515_duplicated.xml'\n",
    "biomass_rxn_id = 'BIOMASS_Ec_iML1515_core_75p37M'\n",
    "\n",
    "# Hyperparameters\n",
    "seed = 10\n",
    "np.random.seed(seed=seed)\n",
    "hidden_layers_lag = [50]\n",
    "hidden_layers_flux = [500]\n",
    "num_epochs = 1000\n",
    "x_fold = 3      \n",
    "batch_size = 10\n",
    "patience = 100\n",
    "N_iter = 3\n",
    "run_name = f'{file_name}_{train_test_split}'\n",
    "\n",
    "# Create model\n",
    "model, train_array, train_dev, val_array, val_dev, val_ids = utils.create_model_train_val(\n",
    "    media_file, od_file,\n",
    "    cobra_model_file,\n",
    "    biomass_rxn_id,\n",
    "    x_fold=x_fold, \n",
    "    hidden_layers_lag=hidden_layers_lag, \n",
    "    hidden_layers_flux=hidden_layers_flux, \n",
    "    dropout_rate=0.2,\n",
    "    loss_weight=[0.001, 1, 1, 1], \n",
    "    loss_decay=[0, 0.5, 0.5, 1], \n",
    "    verbose=True,\n",
    "    train_test_split=train_test_split\n",
    ")\n",
    "\n",
    "# Saving for future testing and validation\n",
    "np.savetxt(f'{folder}model/{run_name}_val_array.txt', val_array, fmt='%f')\n",
    "np.savetxt(f'{folder}model/{run_name}_val_dev.txt', val_dev, fmt='%f')\n",
    "np.savetxt(f'{folder}model/{run_name}_val_ids.txt', np.asarray(val_ids), fmt='%d')\n",
    "\n",
    "# Train model\n",
    "for i in range(N_iter):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-3,\n",
    "        decay_steps=280,\n",
    "        decay_rate=0.9,\n",
    "        staircase=True\n",
    "    )\n",
    "    (\n",
    "        (losses_s_v_train, losses_neg_v_train, losses_c_train, losses_drop_c_train),\n",
    "        (losses_s_v_val, losses_neg_v_val, losses_c_val, losses_drop_c_val)\n",
    "    ) = utils.train_model(\n",
    "        model, train_array, val_array=val_array,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        num_epochs=num_epochs, batch_size=batch_size, patience=patience,\n",
    "        verbose=True,\n",
    "        train_test_split=train_test_split,\n",
    "        x_fold=x_fold\n",
    "    )\n",
    "\n",
    "    model_name = f'{folder}model/{run_name}_{str(i)}'\n",
    "    model.save_model(model_name=model_name, verbose=True)\n",
    "\n",
    "    utils.plot_loss('Training S_v', losses_s_v_train, num_epochs, save=\"./figure\")\n",
    "    utils.plot_loss('Training Neg_v', losses_neg_v_train, num_epochs, save=\"./figure\")\n",
    "    utils.plot_loss('Training C', losses_c_train, num_epochs, save=\"./figure\")\n",
    "    utils.plot_loss('Training Drop_c', losses_drop_c_train, num_epochs, save=\"./figure\")\n",
    "\n",
    "    if x_fold > 1:\n",
    "        utils.plot_loss('Validation S_v', losses_s_v_val, num_epochs, save=\"./figure\")\n",
    "        utils.plot_loss('Validation Neg_v', losses_neg_v_val, num_epochs, save=\"./figure\")\n",
    "        utils.plot_loss('Validation C', losses_c_val, num_epochs, save=\"./figure\")\n",
    "        utils.plot_loss('Validation Drop_c', losses_drop_c_val, num_epochs, save=\"./figure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad4a54a-1617-438c-91d5-9bf31368aec0",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cddc70-d361-4429-a398-36708f5a7a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import sklearn\n",
    "import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "metabolite_ids = [\n",
    "'glc__D_e', 'xyl__D_e', 'succ_e', 'ala__L_e', 'arg__L_e', 'asn__L_e', 'asp__L_e',\n",
    "'cys__L_e', 'glu__L_e', 'gln__L_e', 'gly_e', 'his__L_e', 'ile__L_e', 'leu__L_e',\n",
    "'lys__L_e', 'met__L_e', 'phe__L_e', 'pro__L_e', 'ser__L_e', 'thr__L_e', 'trp__L_e',\n",
    "'tyr__L_e', 'val__L_e', 'ade_e', 'gua_e', 'csn_e', 'ura_e', 'thymd_e', 'BIOMASS'\n",
    "]\n",
    "\n",
    "train_test_split = 'forecast' # 'forecast' or 'medium'\n",
    "folder = './'\n",
    "file_name = 'M28_OD_20'\n",
    "N_iter = 3\n",
    "run_name = f'{file_name}_{train_test_split}'\n",
    "OD = True # when True biomass concentration transformed in OD\n",
    "plot =  'growth' # 'growth' or 'substrate'\n",
    "\n",
    "# Load\n",
    "val_array = np.loadtxt(f'{folder}model/{run_name}_val_array.txt', dtype=float)\n",
    "val_dev = np.loadtxt(f'{folder}model/{run_name}_val_dev.txt', dtype=float)\n",
    "val_ids = np.loadtxt(f'{folder}model/{run_name}_val_ids.txt', dtype=int)\n",
    "if val_array is None:\n",
    "    raise ValueError(f'Validation file not found: {folder}model/{run_name}_val_array.txt')\n",
    "\n",
    "# Predict\n",
    "Pred, Ref, Pred_bio, Ref_bio = {}, {}, {}, {}\n",
    "for i in range(N_iter):\n",
    "    model_name = f'{folder}model/{run_name}_{str(i)}'\n",
    "    model = utils.MetabolicModel.load_model(model_name=model_name, verbose=False)\n",
    "    model.metabolite_ids = metabolite_ids if len(model.metabolite_ids) == 0 else model.metabolite_ids\n",
    "    pred, ref = utils.predict_on_val_data(model, val_array, verbose=False) # 1, 86, 157\n",
    "    pred, ref = np.asarray(pred), np.asarray(ref)\n",
    "    Pred[i], Ref[i] = pred, ref\n",
    "Pred , Ref = np.asarray(list(Pred.values())), np.asarray(list(Ref.values()))\n",
    "R2 = utils.r2_growth_curve(Pred, Ref, OD=OD)\n",
    "\n",
    "print(f'Model: {run_name}  R2 = {np.mean(R2):.2f}±{np.std(R2):.2f} Median = {np.median(R2):.2f}')\n",
    "title = f\"R2 Histogram {train_test_split}\"\n",
    "utils.plot_similarity_distribution(title, R2, save=\"./figure\")\n",
    "\n",
    "# Plot\n",
    "if plot == 'growth':\n",
    "    utils.plot_predicted_reference_growth_curve(\n",
    "    times=model.times,\n",
    "    Pred=Pred, Ref=Ref, val_dev=val_dev,\n",
    "    OD=OD,R2=R2,\n",
    "    train_time_steps=model.train_time_steps if hasattr(model, \"train_time_steps\") else 0,\n",
    "    experiment_ids=list(val_ids),\n",
    "    run_name=run_name,\n",
    "    train_test_split=train_test_split,\n",
    "    save=\"./figure\"\n",
    "    )\n",
    "elif plot == 'substrate':\n",
    "    utils.plot_predicted_biomass_and_substrate(\n",
    "    model.times, Pred,\n",
    "    experiment_ids=list(val_ids),\n",
    "    metabolite_ids=list(model.metabolite_ids),\n",
    "    run_name=run_name,\n",
    "    train_test_split=train_test_split,\n",
    "    save=\"./figure\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff402f-dfaa-43bf-b3fc-7e5f4d94546b",
   "metadata": {},
   "source": [
    "## Parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2304aade-e70a-4951-94b6-9bc442617702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from itertools import product\n",
    "import json\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "train_test_split = 'forecast' # 'forecast' or 'medium'\n",
    "folder = './'\n",
    "media_file = folder+'data/'+'M28_media.csv'\n",
    "od_file = folder+'data/'+'M28_OD_20.csv'\n",
    "cobra_model_file = folder+'data/'+'iML1515_duplicated.xml'\n",
    "biomass_rxn_id = 'BIOMASS_Ec_iML1515_core_75p37M'\n",
    "hidden_layers_lag = [50]\n",
    "hidden_layers_flux = [500]\n",
    "\n",
    "# Parameter search spaces\n",
    "l = [0.0001, 0.001, 0.01, 0.1, 1.0, 1, 2]\n",
    "k = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "l1, l2, l3, l4 = l.copy(), l.copy(), l.copy(), l.copy() \n",
    "k1, k2, k3, k4 = k.copy(), k.copy(), k.copy(), k.copy()\n",
    "N_search = 25\n",
    "param_grid = list(product(l1, k2, k3, k4))\n",
    "random.seed(1)\n",
    "param_samples = random.sample(param_grid, min(N_search, len(param_grid)))\n",
    "short_num_epochs = 500\n",
    "batch_size = 10\n",
    "patience = 30\n",
    "x_fold = 3\n",
    "results = []\n",
    "l1_, l2_, l3_, l4_ = 0.001, 1, 1, 1\n",
    "k1_, k2_, k3_, k4_ = 0, 0.5, 0.5, 1\n",
    "\n",
    "for idx, (l1_, k2_, k3_, k4_) in enumerate(param_samples):\n",
    "    # Create train and val sets\n",
    "    model, train_array, train_dev, val_array, val_dev, val_ids = utils.create_model_train_val(\n",
    "        media_file, od_file, cobra_model_file, biomass_rxn_id,\n",
    "        x_fold=x_fold,\n",
    "        hidden_layers_lag=hidden_layers_lag,\n",
    "        hidden_layers_flux=hidden_layers_flux,\n",
    "        dropout_rate=0.2,\n",
    "        loss_weight =[l1_, l2_, l3_, l4_],\n",
    "        loss_decay  =[k1_, k2_, k3_, k4_],\n",
    "        verbose=False,\n",
    "        train_test_split=train_test_split\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=1e-3, decay_steps=280, decay_rate=0.9, staircase=True\n",
    "    )\n",
    "    (\n",
    "        (losses_s_v_train, losses_neg_v_train, losses_c_train, losses_drop_c_train),\n",
    "        (losses_s_v_val, losses_neg_v_val, losses_c_val, losses_drop_c_val)\n",
    "    ) = utils.train_model(\n",
    "        model, train_array, val_array=val_array,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        num_epochs=short_num_epochs, batch_size=batch_size, patience=patience,\n",
    "        verbose=False,\n",
    "        train_test_split=train_test_split,\n",
    "        x_fold=x_fold\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    pred, ref = utils.predict_on_val_data(model, val_array, verbose=False)\n",
    "    pred, ref = utils.concentration_to_OD(pred),  utils.concentration_to_OD(ref) \n",
    "    pred = np.expand_dims(pred, axis=0)\n",
    "    ref = np.expand_dims(ref, axis=0)\n",
    "    R2 = utils.r2_growth_curve(pred, ref, OD=True)\n",
    "    R2_mean, R2_std = np.mean(R2), np.std(R2)\n",
    "    \n",
    "    print(f'*** Testing λ={[l1_, l2_, l3_, l4_]}, k={[k1_, k2_, k3_, k4_]} ({idx+1}/{len(param_samples)}) '\\\n",
    "          f'R2: {R2_mean:.3f} ± {R2_std:.3f}')\n",
    "    \n",
    "    # Record\n",
    "    results.append({\n",
    "        \"l1\": l1_,\n",
    "        \"l2\": l2_,\n",
    "        \"l3\": l3_,\n",
    "        \"l4\": l4_,\n",
    "        \"k1\": k1_,\n",
    "        \"k2\": k2_,\n",
    "        \"k3\": k3_,\n",
    "        \"k4\": k4_,\n",
    "        \"R2_mean\": R2_mean,\n",
    "        \"R2_std\":  R2_std \n",
    "    })\n",
    "\n",
    "results = sorted(results, key=lambda x: x[\"R2_mean\"], reverse=True)\n",
    "print(\"\\nFull Table: Hyperparameter Search Results\")\n",
    "print(\"l1\\tl2\\tl3\\tl4\\tk1\\tk2\\tk3\\tk4\\tR2-mean\\tR2-std\")\n",
    "for res in results:\n",
    "    print(f\"{res['l1']}\\t{res['l2']}\\t{res['l3']}\\t{res['l4']}\\t{res['k1']}\\t{res['k2']}\\t{res['k3']}\\t{res['k4']}\\t{res['R2_mean']:.3f}\\t{res['R2_std']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bcaa64-80c8-4151-a828-4d25b3a423ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (M4)",
   "language": "python",
   "name": "m4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
